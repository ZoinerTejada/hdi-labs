{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Batch & ETL processing of Big Data with Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdventureWorks is an e-commerce retailer who is looking to improve how they manage the data assets produced by their platform. As a starting point they would like collect their data in a manner that enables easier exploration and prepares the data for downstream analytics processes that can yield new insights. Adventurworks has asked you to process and prepare their flat file data into a tabular format that offers better query perfomrance and can be queried using SQL.\n",
    "\n",
    "In the lab you will learn how to use Spark SQL (and PySpark) to batch process a 10GB text file dataset, quickly explore its content, identify issues with the data, clean and format the data and load it into Hive tables to support downstream analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "Before attempting this lab, make sure you:\n",
    "* Have provisioned an HDInsight 3.6 cluster with Spark 2.1 \n",
    "* Have copied the retaildata to the default storage for your Spark cluster.\n",
    "* Are running these notebooks from your HDInsight cluster.\n",
    "These steps are described in the lab-preqs guide included with these notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by examing the weblogs included in the retaildata dataset. Each days worth of data is stored under a folder path of month/day in a file called weblog.txt as the following output illustrates. The files sizes (the third column) are in expressed as M for MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls -h /retaildata/rawdata/weblognew/1/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the Python modules and functions we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint, datetime\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import date_format,unix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the header row along with one row data of the weblog files to get a sense for the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"/retaildata/rawdata/weblognew/1/1/weblog.txt\")\n",
    "lines.show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's enough to give us a sense of what these files contain. Let's move on to trying to parse these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and stage the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will try to read the data directly as a CSV, letting Spark infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: invoke the spark.read.csv function with a path to a single weblog.txt. \n",
    "df = spark.#Complete this line# \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly a dataset with one column of type string is not what we want. If you look at the lines we outputted above, it should be make sense why we got this. The weblog data has one row per event, where each column in a row is separated by a pipe (|) character. \n",
    "\n",
    "Let's see how we can address this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle input format issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will read the file inferring the schema from the data, and tell Spark to use the first row to provide the column names. See the [documentation for the csv()](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) function in the DataFrameReader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 1. modify your path to the weblogs so that it uses a wildcard (*) to \n",
    "# include all months and all days for the data available, \n",
    "# TODO 2. use the sep parameter to indicate the row seperator character\n",
    "# TODO 3. use the header parameter to treat the first row of each files as the column names  \n",
    "df = spark.read.csv(#TODO 1,#TODO 2,#TODO 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so far so good. What was the schema that Spark inferred? Run the following to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that above will treat all columns as strings. \n",
    "However, we have columns that should be \n",
    "   * numeric: UserId, ProductID, Quantity, Price, TotalPrice, PageStopDuration\n",
    "   * string: TransactionDate (we'll worry about treating this as a timestamp data type later)\n",
    "   \n",
    "Let's fix that by providing a schema. See [this description of creating a StructField](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.types.StructField) and see [this link](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.types.DataType) for a list of data types. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weblogs_schema = StructType([\n",
    "        StructField('UserId',LongType(),False), \n",
    "        StructField('SessionId', StringType()), \n",
    "        StructField('ProductId', IntegerType()), \n",
    "        StructField('Quantity', IntegerType()), \n",
    "        StructField('Price', DoubleType()), \n",
    "        StructField('TotalPrice', DoubleType()), \n",
    "        StructField('ReferralURL', StringType()), \n",
    "        StructField('PageStopDuration', IntegerType()), \n",
    "        StructField('Action', StringType()), \n",
    "        StructField('TransactionDate', StringType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use this schema as an input to the csv function and then examine the new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: provide the weblogs_schema as the value for the schema parameter in the call to csv().\n",
    "df = spark.read.csv(\"/retaildata/rawdata/weblognew/*/*/weblog.txt\",\n",
    "                    #TODO,\n",
    "                    sep=\"|\",\n",
    "                    header=True\n",
    "                    )\n",
    "pprint.pprint(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the data still looks good with this new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and format the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we glossed over treating the TransactionDate field as a timestamp type, choosing to leave it as string type instead. Let's work to fix that. First, let's look at some sample values to get a sense of the shape of the TransactionDate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select(\"TransactionDate\").show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above listing of transaction dates, the timestamp is represented in a human friendly way, but it is not the ideal format for use when querying the data. We would prefer to see this TransactionDate in the form:\n",
    "\n",
    "`2016-01-02 22:05:55`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pyspark.sql.functions module contains a unix_timestamp function that might help us with the parsing of the strings in the TransactionDate column.\n",
    "\n",
    "Lets read the documentation (aka the docstring) about the function. All functions in the PySpark API are documented this way, so you can use this same approach to learn about any new function you encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(unix_timestamp.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the unix_timestamp inside of select to operate on the column of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Author the format string to parse the source data.\n",
    "df.select(\n",
    "    \"TransactionDate\",\n",
    "    unix_timestamp(\"TransactionDate\", #TODO).alias(\"NewDate\").cast(\"timestamp\")\n",
    ").show(5, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. The only issue is the values in NewDate appear to have a fractional second component like 00:24:00**.0** that we do not want. So it looks like we now need to format the timestamp to use our preferred format. \n",
    "\n",
    "The date_format function can help us with this. Let's look at the documentation for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(date_format.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date_format function relies on a format string pattern. You need to provide a custom format string for the timestamps used by the weblogs dataset, can you figure out the string? Hint: under the covers Spark uses the SimpleDateFormat class to format dates, see the [SimpleDateFormat](http://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html) documentation. \n",
    "\n",
    "Test out your format string on the following example date to make sure you get it right. If your are getting a null value back from date_format, that means it was unable to parse the input using the pattern you supplied (and so there's something still not quite right with your pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Format the timestamp by providing a format string that yields values like 2016-01-01 14:10:00.\n",
    "df.select(\n",
    "    \"TransactionDate\",\n",
    "    date_format(\n",
    "        unix_timestamp(\"TransactionDate\", \"M/d/yyyy h:mm:ss a\").cast(\"timestamp\"),\n",
    "        #TODO).alias(\"NewDate\")\n",
    ").show(5, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you think you have the date formatting figured out, it's time to try it against the whole dataset to see how it works. Remember, there are two potential issues here. Your format string may not be quite right OR the data itself might be garbage data. \n",
    "\n",
    "The following query runs across all of the data to find the instances where we were unable to parse and reformat the date. \n",
    "\n",
    "**Note:** this query will take about 4 minutes to complete since it is running across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(\n",
    "    df.select(\"TransactionDate\",\n",
    "          date_format(\n",
    "            unix_timestamp(\"TransactionDate\",\"M/d/yyyy h:mm:ss a\").cast(\"timestamp\"),\n",
    "            \"yyyy-MM-dd HH:mm:ss\").alias(\"date\")\n",
    "         ).where(\"date IS NULL\").take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are 3 rows that have TransactionDates that we could not parse. Rather than try to fix these values, we will just delete the rows from the cleaned dataset. \n",
    "\n",
    "In the following we apply a where clause to filter out the above three rows whose TransactionDate could not be parsed/formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the SQL where filter clause to eliminate rows where CleanedTransactionDate is NULL\n",
    "df2 = df.select(\"*\",\n",
    "          date_format(\n",
    "            unix_timestamp(\"TransactionDate\",\"M/d/yyyy h:mm:ss a\").cast(\"timestamp\"),\n",
    "            \"yyyy-MM-dd HH:mm:ss\").alias(\"CleanedTransactionDate\")\n",
    "         ).where(#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date formatting for the new columns is in place, but let's take a look at the schema that resulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, while the value of CleanedTransactionDate is now better formatted, it is still being treated as having a string data type. We can fix that by casting the new column to be of type ```timestamp```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_df = df.select(\"*\",\n",
    "          date_format(\n",
    "            unix_timestamp(\"TransactionDate\",\"M/d/yyyy h:mm:ss a\").cast(\"timestamp\"),\n",
    "            \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\").alias(\"CleanedTransactionDate\")\n",
    "         ).where(\"CleanedTransactionDate IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the resulting schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our CleanedTransactionDate column will actually use the timestamp date type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a view to explore the data using SQL without moving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a temporary view that enables us to use SQL statements to query our cleaned up dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Invoke the createOrReplaceTempView function on your cleaned_df object \n",
    "# to create a view named \"weblogs_view\".\n",
    "cleaned_df.#TODO( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ```%%sql``` cell magic to switch the mode of the cell from running Python to running SQL. Everything on the lines below ```%%sql``` is SQL that SparkSQL will execute for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM weblogs_view limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the prepared data to persistent Hive tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above view is temporary and will eventually be deleted. If we want our cleansed data to remain permanently, we should export it to a Hive table. \n",
    "\n",
    "Run the following to copy the data from source location into the Hive warehouse location. This will create a Hive managed table you can query with Hive as well as with Spark. If you get an error in the below, check your schema and the parameters you passed to the csv function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO 1: Set the mode to overwrite so when this cell is re-run it re-creates the table and data.\n",
    "#TODO 2: Use the saveAsTable method to create a Hive table called \"weblogs\".\n",
    "cleaned_df.write.mode(#TODO 1).#TODO 2(\"weblogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to see how files behind the Hive table land in Azure Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls -h /hive/warehouse/weblogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that our source data, which was originally stored in text files, is now stored as a series of parquet files. \n",
    "\n",
    "The Hive table by default is saved out in the Parquet format, which as we'll see momentarily transparently provides some performance improvement over querying the raw text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify you can query the Hive table using Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query a Hive table using Spark SQL in the familiar way. Let's start by checking how many rows our new Hive table contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Author a SQL query that counts the number of rows in the weblogs table\n",
    "spark.sql(#TODO).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also spot check the date ranges it encompasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT Max(CleanedTransactionDate), Min(CleanedTransactionDate) FROM weblogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM weblogs LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice how much faster the queries seemed to run against our Hive table? This was all the result of saving the table's data to Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Users table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdventureWorks also has a users dataset in flat files that provides details about the users whose events are captured in the weblogs. You will need to prepare this table and create a Hive table for this data just you did for the weblogs.\n",
    "\n",
    "In this case, however, you will take a different approach to parsing the data. It's not always the case that the source data is shaped the way you need, and you need to apply parsing beyond that supplied by the spark.read.csv function. In this case, we need to handle the parsing of certain columns of data by applying numeric and timestamp parsing, but this approach generalizes to any data set where you need to shape the data field by field and line by line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by examining one row of the data to get a sense of the shape. Note that these files do not include a header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"/retaildata/rawdata/UserFile/\")\n",
    "lines.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above output DataFrame, observe that the one eaxmple line of the file contains comma separated values. What's more, none of the invidual values within a column appear to have commas that could throw off any simple parsing. \n",
    "\n",
    "Observe that there are 20 fields in the row of data, where each field is separated from the next by a comma.\n",
    "\n",
    "Parse the files by splitting each line into fields by splitting on a \",\" and create as output a collection of python tuples. Note that you will need to access the underlying RDD of the DataFrame to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = lines.rdd.map(lambda l: l.value.split(\",\"))\n",
    "rows = fields.map(lambda p: (p[0], p[1], p[2], p[3], p[4], p[5], \n",
    "                            p[6], p[7], p[8], p[9], p[10], \n",
    "                            p[11], p[12], p[13], p[14], p[15],\n",
    "                            p[16], p[17], p[18], p[19]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following and confirm that the data appears to be split correctly for the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides the schema of the data for the data, as provided by AdventureWorks. In the cell that follows, you will parse and load the rows of the data line by line using a Python lambda function and then apply this schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_schema = StructType([\n",
    "        StructField('id',IntegerType(),False), \n",
    "        StructField('LoginMd5', StringType()), \n",
    "        StructField('Email', StringType()), \n",
    "        StructField('FirstName', StringType()), \n",
    "        StructField('PictureLarge', StringType()), \n",
    "        StructField('LastName', StringType()), \n",
    "        StructField('LoginSha1', StringType()), \n",
    "        StructField('Username', StringType()), \n",
    "        StructField('Title', StringType()), \n",
    "        StructField('Gender', StringType()), \n",
    "        StructField('LoginSalt', StringType()),\n",
    "        StructField('Phone', StringType()),\n",
    "        StructField('Password', StringType()),\n",
    "        StructField('LoginSha256', StringType()),\n",
    "        StructField('PictureThumbnail', StringType()),\n",
    "        StructField('Age', IntegerType()),\n",
    "        StructField('Cell', StringType()),\n",
    "        StructField('BirthDate', TimestampType()),\n",
    "        StructField('Registered', TimestampType()),\n",
    "        StructField('PictureMedium', StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to apply the schema to the rows as you have them now. What is the error? **HINT: You are looking for a TypeError**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: call the createDataFrame method on the spark object, providing the rows you parsed \n",
    "# and their schema\n",
    "users_df = spark.#TODO( , )\n",
    "users_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the schema provided by Adventurworks most of the fields are strings, except for:\n",
    "* id which should be an integer\n",
    "* age which should be an integer\n",
    "* birthdate which should be a timestamp\n",
    "* registered which whould be a timestamp\n",
    "\n",
    "Looking at our example row above, we see an example value for each:\n",
    "* id: '9858'\n",
    "* Age: '53'\n",
    "* BirthDate: '1964-11-24 10:54:00.000'\n",
    "* Registered: '2016-10-11 07:38:00'\n",
    "\n",
    "In the cell below, complete the Python parsing code so that each of string values is parsed succesfully. For the timestamp values, refer to [this table for the format codes](https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9876 53 1964-11-24 10:54:00 2016-10-11 07:38:00"
     ]
    }
   ],
   "source": [
    "#TODO 1: parse an int from the string id\n",
    "#TODO 2: parse an int from the string age\n",
    "#TODO 3: provide the format string to parse a timestamp from the string BirthDate \n",
    "#TODO 4: provide the format string to parse a timestamp from the string Registered\n",
    "print(\n",
    "    #TODO 1('9876'),\n",
    "    #TODO 2('53'),\n",
    "    datetime.datetime.strptime('1964-11-24 10:54:00.000', #TODO 3),\n",
    "    datetime.datetime.strptime('2016-10-11 07:38:00', #TODO 4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify the lambda to apply your parsing function to the appropriate cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TODO 1: parse the age field into an int\n",
    "#TODO 2: parse the Registered field into a timestamp\n",
    "fields = lines.rdd.map(lambda l: l.value.split(\",\"))\n",
    "rows = fields.map(lambda p: (int(p[0]), p[1], p[2], p[3], p[4], p[5], \n",
    "                            p[6], p[7], p[8], p[9], p[10], \n",
    "                            p[11], p[12], p[13], p[14], #TODO 1(p[15]),\n",
    "                            p[16], \n",
    "                            datetime.datetime.strptime(p[17], \"%Y-%m-%d %H:%M:%S.%f\"), \n",
    "                            #TODO 2(p[18], ),\n",
    "                            p[19]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a new DataFrame and verify your first row was handled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_df = spark.createDataFrame(rows, users_schema)\n",
    "users_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, create a temporary view for this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_df.createOrReplaceTempView(\"users_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, query the view to summarize the table and spot check your data parsing is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(id), min(id), max(id), \n",
    "        min(age), max(age), min(BirthDate), max(BirthDate), \n",
    "        min(Registered), max(Registered)\n",
    "from users_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save this view as new Hive table called users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_df.write.mode(\"overwrite\").saveAsTable(\"users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't need the weblogs_view anymore, we can drop it. Because it is a temporary view it will get dropped when our Spark session ends, but we can use the following to drop it sooner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"weblogs_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lab you have learned how to use Spark SQL (and PySpark) to batch process a 10GB text file dataset, quickly explore its content, identify issues with the data, clean and format the data and load it into a Hive table to support downstream analytics. Specifically you:\n",
    "* Loaded text flat files into a Spark SQL DataFrame.\n",
    "* Added a new column to the DataFrame that stored the TransactionData in the format desired.\n",
    "* Created a temporary view from the DataFrame.\n",
    "* Copied the data from the DataFrame into a Hive table and stored the data using Parquet.\n",
    "* Verified you can query the Hive table and observed the increase in query speed that came from saving the data in Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
