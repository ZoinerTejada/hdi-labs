{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7 - Extending the cluster with HDInsight Applications\n",
    "\n",
    "AdventureWorks is interested in using HDInsight applications for extending the capabilities their cluster. They are interested in two applications, H2O Sparkling Water and Apache Solr. H2O will provide machine learning and predictive analytics, while Solr will provide enterprise search capabilities.\n",
    "\n",
    "They have provided you with the tables for users, products, and weblogs that contain all the data you need. You will build and train a deep learning model using H2O Sparkling Water, combining the capabilities of Spark with H2O. Then, you will use Solr to add search capabilities to the AdventureWorks cluster.\n",
    "\n",
    "In this lab you will learn how to extend an existing HDInsight cluster by installing both third-party and custom applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requistes\n",
    "\n",
    "Before attempting this lab, make sure you:\n",
    "* Have provisioned an HDInsight 3.6 cluster with Spark 2.1.\n",
    "* Have copied the retaildata to the default storage for your Spark cluster.\n",
    "* Are running these notebooks from your HDInsight cluster.\n",
    "\n",
    "These steps are described in the lab-preqs guide, included with these notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install H2O Sparkling Water\n",
    "![H2O](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/h2o.png)\n",
    "\n",
    "Through H2O’s AI platform and its Sparkling Water solution, users can combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark, as well as drive computation from Scala/R/Python and utilize the H2O Flow UI, providing an ideal machine learning platform for application developers.\n",
    "\n",
    "H2O is an open source, in-memory, distributed, fast, and scalable machine learning and predictive analytics platform that allows you to build machine learning models on big data and provides easy productionalization of those models in an enterprise environment.\n",
    "\n",
    "Sparkling Water allows users to combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark. With Sparkling Water, users can drive computation from Scala/R/Python and utilize the H2O Flow UI, providing an ideal machine learning platform for application developers.\n",
    "\n",
    "H2O can be installed on an existing HDInsight cluster, or can be included as part of a new cluster creation. For our purposes, we are going to install on our existing cluster.\n",
    "\n",
    "For your cluster blade in the Azure portal:\n",
    "1. Select the `Applications` option under Configuration, or on the overview blade.\n",
    "    ![Applications](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/applications-access.png)\n",
    "2. On the Applications blade, select **H2O Artificial Intelligence for HDInsight**, under Available applications. (See below if the H2O application in listed under Unavailable applications)\n",
    "    ![Applications - H2O](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/applications-blade.png)\n",
    "3. Select **Review Legal Terms** on the H2O Artifical Intelligence blade.\n",
    "    ![H2O Blade](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/h2o-blade.png)\n",
    "4. Review the Terms of use, and select **Purchase**.\n",
    "    ![H2O Terms of Use](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/h2o-legal-terms.png)\n",
    "    ![Purchase](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/purchsae.png)\n",
    "5. Click **OK** on the H2O blade, with the Legal terms accepted.\n",
    "    ![H2O Legal Terms Accepted](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/h2o-legal-terms-accepted.png)\n",
    "6. Click **Next** on the Applications blade to install H2O.\n",
    "    a. The installation will take approximately 10 minutes to complete.\n",
    "    b. Once the installation is complete, you can see the application by clicking again on **Applications** on the cluster blade.\n",
    "        ![Installed Apps](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/h2o-installed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If during the installation process above you encountered an issue where the **H2O** application is listed under Unavailable applications, you will need to delete your cluster, and create a new one, including H2O as part of the cluster creation process.\n",
    "![H2O Unavailable](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/h2o-unavailable-apps.png)\n",
    "\n",
    "TODO: List steps perform \"custom\" cluster install, so H2O app can be included during the install process.\n",
    "To delete and create a new cluster, follow the steps below:\n",
    "\n",
    "TODO: Add \"Deploy to Azure\" button to provision a new cluster, with H2O as part of the process...\n",
    "\n",
    "From your cluster blade in the Azure portal:\n",
    "1. Select **Delete**, and select **Yes** to confirm you want to delete the cluster.\n",
    "    ![Delete Cluster](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/delete-cluster.png)\n",
    "2. You will receive a message that the cluster is being deleted.\n",
    "    ![Deleting Cluster Message](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/deleting_cluster.png)\n",
    "3. Now, you will create a new cluster in the same resource group.\n",
    "4. Navigate to the resource group, and click **+ Add**.\n",
    "    ![Resource Group App](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/resource-group-add.png)\n",
    "5. Enter HDI in the search box, and select HDInsight.\n",
    "    ![HDInsight Search](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/hdinsight-search.png)\n",
    "6. Select HDInsight from the list.\n",
    "    ![HDInsight](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/hdinsight-select.png)\n",
    "7. On the HDInsight blade, select **Create**.\n",
    "    ![HDInsight create blade](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/hdinsight-create-blade.png)\n",
    "8. On the HDInsight blade, select **Custom (size, settings, apps)** at the top of the blade, so the H2O application can be installed as part of the creation process.\n",
    "    ![HDInsight Custom Install](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/hdinsight-custom-install.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing H2O on HDInsight, you can use the built-in Jupyter notebooks to write your first H2O on HDInsights applications..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the H2O Cluster\n",
    "Now that H2O is installed, the next thing that needs to be done is to configure the environment. Most of the configurations are already taken care by the system, such as the FLOW UI address, Spark jar location, the Sparkling water egg file, etc., however there are still a few settings that need to configure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Edge node hostname\n",
    "To ensure we are able to connect to the H2O Flow UI, we need to assign the proper value to the  `spark.ext.h2o.announce.rest.url` in the configuration below. To find the correct host_name value, we need to look up the value in the H2O Sparkling Water configuration file.\n",
    "\n",
    "Let's start by importing the Python types we'll need to query the H2O config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1507613647085_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-hdilab.yzcjmk5uxsoepi32miarvyzmib.cx.internal.cloudapp.net:8088/proxy/application_1507613647085_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.11:30060/node/containerlogs/container_1507613647085_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary types for querying the H2O config file.\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, read the H2O config file from its Azure storage location. Be sure to replace the clustername with the appropriate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+\n",
      "|Value                                                              |\n",
      "+-------------------------------------------------------------------+\n",
      "|(\"ed11-hdilab.yzcjmk5uxsoepi32miarvyzmib.cx.internal.cloudapp.net\")|\n",
      "+-------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "clustername = 'hdilabskyle9' # Replace with your HDInsight cluster name\n",
    "config_schema = StructType([\n",
    "        StructField('Name',StringType()), \n",
    "        StructField('Value', StringType())])\n",
    "\n",
    "swconfig = spark.read.csv(\"/HdiApplications/ScriptActionCfgs/%s-h2o-sparklingwater.cfg\" % clustername,\n",
    "                    schema=config_schema,\n",
    "                    sep=\"=\",\n",
    "                    header=False)\n",
    "\n",
    "swconfig.filter(swconfig['Name'] == \"EDGENODE_HOSTS\").select(swconfig['Value']).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the host name value (e.g., ed11-hdilab.fu31bippkliejecocwb1m5yjga.bx.internal.cloudapp.net) from the configuration above, and paste it into the code below, replacing the `<EdgeHostName>` component of the `spark.ext.h2o.announce.rest.url` value. This will ensure you are able to properly connect to H2O Flow UI once H2O Sparkling Water properly is started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the H2O configuration\n",
    "There are four important parameters which must still be configured: \n",
    "1. H2O Flow UI URL (retrieved in the previous step)\n",
    "2. Driver memory\n",
    "3. Executor memory\n",
    "4. The number of executors\n",
    "\n",
    "The driver and executor memory, and number of executors is driven by the number of worker nodes in the cluster. For our case, we have 2 worker nodes, so we will assign 1 as an executor, to ensure we have enough resources for H2O to work properly. You want to ensure the resource utilization for the H2O cluster remains below 75%.\n",
    "\n",
    "> Note that all spark applications deployed using a Jupyter Notebook will have \"yarn-cluster\" deploy-mode. This means that the spark driver node will be allocated on any worker node of the cluster, not on the head nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1507613647085_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-hdilab.yzcjmk5uxsoepi32miarvyzmib.cx.internal.cloudapp.net:8088/proxy/application_1507613647085_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.11:30060/node/containerlogs/container_1507613647085_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{u'numExecutors': 1, u'executorMemory': u'20G', u'kind': 'pyspark', u'conf': {u'spark.scheduler.minRegisteredResourcesRatio': u'1', u'spark.task.maxFailures': u'1', u'spark.yarn.am.extraJavaOption': u'-XX:MaxPermSize=384m', u'maximizeResourceAllocation': u'true', u'spark.yarn.max.executor.failures': u'1', u'spark.jars': u'/H2O-Sparkling-Water-files/sparkling-water-assembly-all.jar', u'spark.ext.h2o.announce.rest.url': u'http://ed11-hdilab.yzcjmk5uxsoepi32miarvyzmib.cx.internal.cloudapp.net:5000/flows', u'spark.submit.pyFiles': u'/H2O-Sparkling-Water-files/pySparkling.egg', u'spark.locality.wait': u'3000'}, u'driverMemory': u'10G'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1507613647085_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-hdilab.yzcjmk5uxsoepi32miarvyzmib.cx.internal.cloudapp.net:8088/proxy/application_1507613647085_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.11:30060/node/containerlogs/container_1507613647085_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.ext.h2o.announce.rest.url\": \"http://ed11-hdilab.yzcjmk5uxsoepi32miarvyzmib.cx.internal.cloudapp.net:5000/flows\",\n",
    "        \"spark.jars\":\"/H2O-Sparkling-Water-files/sparkling-water-assembly-all.jar\",\n",
    "        \"spark.submit.pyFiles\":\"/H2O-Sparkling-Water-files/pySparkling.egg\",\n",
    "        \"spark.locality.wait\":\"3000\",\n",
    "        \"spark.scheduler.minRegisteredResourcesRatio\":\"1\",\n",
    "        \"spark.task.maxFailures\":\"1\",\n",
    "        \"spark.yarn.am.extraJavaOption\":\"-XX:MaxPermSize=384m\",\n",
    "        \"spark.yarn.max.executor.failures\":\"1\",\n",
    "        \"maximizeResourceAllocation\": \"true\"\n",
    "    },\n",
    "    \"driverMemory\":\"10G\",\n",
    "    \"executorMemory\":\"20G\",\n",
    "    \"numExecutors\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate the H2O Context\n",
    "\n",
    "To start the H2O cluster, we call `h2o_context = pysparkling.H2OContext.getOrCreate(sc)`. This initiates an H2O context on top of Spark, so the default spark context can recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://10.0.0.11:54321... successful.\n",
      "--------------------------  -------------------------------\n",
      "H2O cluster uptime:         11 secs\n",
      "H2O cluster version:        3.10.4.3\n",
      "H2O cluster version age:    6 months and 9 days !!!\n",
      "H2O cluster name:           sparkling-water-yarn_1937546603\n",
      "H2O cluster total nodes:    1\n",
      "H2O cluster free memory:    17.78 Gb\n",
      "H2O cluster total cores:    4\n",
      "H2O cluster allowed cores:  4\n",
      "H2O cluster status:         accepting new members, healthy\n",
      "H2O connection url:         http://10.0.0.11:54321\n",
      "H2O connection proxy:\n",
      "H2O internal security:      False\n",
      "Python version:             2.7.12 final\n",
      "--------------------------  -------------------------------"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pysparkling, h2o\n",
    "import os\n",
    "os.environ[\"PYTHON_EGG_CACHE\"] = \"~/\"\n",
    "\n",
    "h2o_context = pysparkling.H2OContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O Flow\n",
    "Once the H2O Cluster is up and running, you can open H2O Flow by going to `https://<ClusterName>-h2o.apps.azurehdinsight.net:443`. This can also be accessed by selecting `Applications` on your HDInsight blade in the Azure portal, then selecting the `Portal` link next to the `h2o-sparklingwater` in the applications list.\n",
    "\n",
    "![H2O Flow](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/h2o-flow-link.png)\n",
    "\n",
    "> Note: If the H2O Flow link redirects you to a help page, try clearning your browser cache. If you are still uable to reach it, you likely don't have enough resources on your cluster. Try decreasing the amount of memory assigned in your configuration above, or increasing the number of **Worker nodes** under the **Scale cluster** option on your cluster blade in the Azure Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for modeling\n",
    "Our goal is to create a simple deep learning model for providing predictions about recommended products for users, based on actions, and the user's age and gender. For this, we need to combine data from the weblogs table with data from our users table.\n",
    "\n",
    "Steps:\n",
    "1. Define UDF to transform Action to an integer score (30, 70, 100)\n",
    "2. Define UDF to transform gender to an integer (Male = 1, Female = 2)\n",
    "3. Create a final, scored DataFrame that can be passed to H2O as our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python modules\n",
    "We are going to use Spark SQL to retrieve our data from Hive tables, so first, let's import the python modules needed to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import UserDefinedFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's quickly look at the two datasets we will be using to build our model, weblogs and users. Both of these datasets have already been stored in Hive tables in our storage account. The tables are named `weblogs` and `users`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------+--------+-----+----------+--------------------+----------------+-------+--------------------+----------------------+\n",
      "|UserId|           SessionId|ProductId|Quantity|Price|TotalPrice|         ReferralURL|PageStopDuration| Action|     TransactionDate|CleanedTransactionDate|\n",
      "+------+--------------------+---------+--------+-----+----------+--------------------+----------------+-------+--------------------+----------------------+\n",
      "|  7516|9576e72c-356f-402...|      509|       0| 55.0|       0.0|         contoso.com|             159|Browsed|2/20/2016 10:26:0...|  2016-02-20 22:26:...|\n",
      "|  7516|9576e72c-356f-402...|      482|       0|  3.8|       0.0|http://contoso.co...|              44|Browsed|2/20/2016 10:28:3...|  2016-02-20 22:28:...|\n",
      "|  7516|9576e72c-356f-402...|      494|       0| 27.5|       0.0|http://contoso.co...|              93|Browsed|2/20/2016 10:29:2...|  2016-02-20 22:29:...|\n",
      "|  7516|9576e72c-356f-402...|      513|       0|  8.0|       0.0|http://contoso.co...|             173|Browsed|2/20/2016 10:30:5...|  2016-02-20 22:30:...|\n",
      "|  7516|9576e72c-356f-402...|      474|       0|  5.0|       0.0|http://contoso.co...|              13|Browsed|2/20/2016 10:33:4...|  2016-02-20 22:33:...|\n",
      "+------+--------------------+---------+--------+-----+----------+--------------------+----------------+-------+--------------------+----------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Weblogs data\n",
    "weblogs_df = spark.sql(\"SELECT * FROM weblogs\")\n",
    "weblogs_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+--------------------+--------+--------------------+---------------+-----+------+---------+--------------+--------+--------------------+--------------------+---+--------------+--------------------+--------------------+--------------------+\n",
      "|  id|            LoginMd5|Email|FirstName|        PictureLarge|LastName|           LoginSha1|       Username|Title|Gender|LoginSalt|         Phone|Password|         LoginSha256|    PictureThumbnail|Age|          Cell|           BirthDate|          Registered|       PictureMedium|\n",
      "+----+--------------------+-----+---------+--------------------+--------+--------------------+---------------+-----+------+---------+--------------+--------+--------------------+--------------------+---+--------------+--------------------+--------------------+--------------------+\n",
      "|9858|6952c3958a740dc51...|  NaN|    tiago|https://randomuse...|ekelmans|73600c23029c824bb...|    blueswan862|   mr|  Male| gjzhB97J|(107)-025-5278|   zhong|1737ae69db2499bec...|https://randomuse...| 53|(670)-009-2361|1964-11-24 10:54:...|2016-10-11 07:38:...|https://randomuse...|\n",
      "|9859|26fcabfdc5be96ef5...|  NaN|   rasmus|https://randomuse...|    ramo|fa08289b3fed5f185...|  smallmouse340|   mr|  Male| dA86NpJx|    09-668-830|stiletto|285ec2e4d7d02dbd0...|https://randomuse...| 59| 046-374-16-05|1958-01-18 21:26:...|2007-06-08 11:03:...|https://randomuse...|\n",
      "|9860|7e18ab469ef67efb1...|  NaN|    belen|https://randomuse...|    moya|df80672d16de3322c...|  heavygoose883|   ms|Female| cF6KT0J3|   968-061-272| monique|8acd54b912a850464...|https://randomuse...| 20|   608-109-987|1997-06-21 13:22:...|2004-08-17 17:58:...|https://randomuse...|\n",
      "|9861|38b8dbb8cebdd29bb...|  NaN| gregorio|https://randomuse...|    cruz|c7a72c9698fe78347...|   smallswan342|   mr|  Male| xLWrwAZS|   955-047-150|  leelee|00d65b119909f9416...|https://randomuse...| 44|   623-541-744|1973-10-06 17:18:...|2008-05-03 20:38:...|https://randomuse...|\n",
      "|9862|965bb7b0c4f9d6102...|  NaN|    perry|https://randomuse...|    wade|37c354180cb6b80a6...|smallladybug419|   mr|  Male| v2BiaC1I|  051-061-1555|  python|92296af94b837ac0a...|https://randomuse...| 64|  081-420-7174|1953-08-19 12:32:...|2003-09-29 01:26:...|https://randomuse...|\n",
      "+----+--------------------+-----+---------+--------------------+--------+--------------------+---------------+-----+------+---------+--------------+--------+--------------------+--------------------+---+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Users data\n",
    "users_df = spark.sql(\"SELECT * FROM users\")\n",
    "users_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the output from the queries above, there are a lot of fields in both datasets that are not needed for our model. Now, we want to create a combined DataFrame containing only the fields we want for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+---------+-------+\n",
      "|UserId|Gender|Age|ProductId| Action|\n",
      "+------+------+---+---------+-------+\n",
      "|  7516|  Male| 51|      509|Browsed|\n",
      "|  7516|  Male| 51|      482|Browsed|\n",
      "|  7516|  Male| 51|      494|Browsed|\n",
      "|  7516|  Male| 51|      513|Browsed|\n",
      "|  7516|  Male| 51|      474|Browsed|\n",
      "+------+------+---+---------+-------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Combine only needed columns for the two tables for our model\n",
    "weblogsWithUsers_df = spark.sql(\"SELECT w.UserId, u.Gender, u.Age, w.ProductId, w.Action FROM weblogs w JOIN users u ON w.UserId = u.Id\")\n",
    "weblogsWithUsers_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Munging with the Spark API\n",
    "Now that we have the data we need, there are a few modifications we need to make in order to get the table ready to be used for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create UDFs to transform data\n",
    "Let's create a couple of User Defined Functions to handle transforming our Action and Gender fields into the proper structure for our model.\n",
    "\n",
    "> Note: We are also converting the Acore and Gender columns to IntegerType() in the process.\n",
    "\n",
    "##### Assign numeric Score to Action\n",
    "Next, we are going to assign scores to the Action field in the weblogs frame. We begin by defining how we want to weigh the implicit rating described by the action field in the weblogs table. An implicit rating occurs here because a user is not explictly providing a rating (e.g., they never say \"I rate this product 4 out of 5 stars\". Instead we will infer their rating by virtue of their action. \n",
    "\n",
    "A product that is browsed gets 30 points, a product that is added to the cart gets 70 points and a product that is purchased gets 100 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UDF to map action values to a score\n",
    "def action_to_score(col):\n",
    "    if col == \"Browsed\":\n",
    "        return 30\n",
    "    elif col == \"Add To Cart\":\n",
    "        return 70\n",
    "    elif col == \"Purchased\":\n",
    "        return 100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "map_action_to_score = UserDefinedFunction(action_to_score, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the UDF to our table, to get scores for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+---------+-------+-----+\n",
      "|UserId|Gender|Age|ProductId| Action|Score|\n",
      "+------+------+---+---------+-------+-----+\n",
      "|  7516|  Male| 51|      509|Browsed|   30|\n",
      "|  7516|  Male| 51|      482|Browsed|   30|\n",
      "|  7516|  Male| 51|      494|Browsed|   30|\n",
      "|  7516|  Male| 51|      513|Browsed|   30|\n",
      "|  7516|  Male| 51|      474|Browsed|   30|\n",
      "+------+------+---+---------+-------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "result_scored = weblogsWithUsers_df.withColumn(\"Score\", map_action_to_score(\"Action\").cast(IntegerType()))\n",
    "result_scored.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Gender to numeric value\n",
    "We also need to convert our gender column to a numeric value for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UDF to map gender to a numeric value\n",
    "def gender_to_int(col):\n",
    "    if col == \"Male\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "map_gender_to_int = UserDefinedFunction(gender_to_int, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By sending Gender data into the UDF, we can convert our Male and Female values into numeric representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+---------+-------+-----+\n",
      "|UserId|Gender|Age|ProductId| Action|Score|\n",
      "+------+------+---+---------+-------+-----+\n",
      "|  7516|     1| 51|      509|Browsed|   30|\n",
      "|  7516|     1| 51|      482|Browsed|   30|\n",
      "|  7516|     1| 51|      494|Browsed|   30|\n",
      "|  7516|     1| 51|      513|Browsed|   30|\n",
      "|  7516|     1| 51|      474|Browsed|   30|\n",
      "+------+------+---+---------+-------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "result_final = result_scored.withColumn(\"Gender\", map_gender_to_int(\"Gender\").cast(IntegerType()))\n",
    "result_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that our `Gender` and `Score` columns are the proper type (`IntegerType`) by looking at the schema of the `result_final` frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserId: long (nullable = true)\n",
      " |-- Gender: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- ProductId: integer (nullable = true)\n",
      " |-- Action: string (nullable = true)\n",
      " |-- Score: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "result_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish result as H2OFrame\n",
    "With the table now in the required shape, we can publish the Spark DataFrame to an H2OFrame, assigning it a friendly name. We will define the columns to use for our H2OFrame as part of the process, so our H2OFrame will include `UserId`, `Age`, `Gender`, `ProductId`, and `Score`. Note, we've also dropped the Action column.\n",
    "\n",
    "Note: This will take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  UserId    Age    Gender    ProductId    Score\n",
      "--------  -----  --------  -----------  -------\n",
      "    7516     51         1          509       30\n",
      "    7516     51         1          482       30\n",
      "    7516     51         1          494       30\n",
      "    7516     51         1          513       30\n",
      "    7516     51         1          474       30\n",
      "    7516     51         1          494       30\n",
      "    7516     51         1          505       30\n",
      "    7516     51         1          514       30\n",
      "    7516     51         1          480       30\n",
      "    7516     51         1          497       30\n",
      "\n",
      "[84916780 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "# Publish Spark DataFrame as H2OFrame with given name\n",
    "final_columns = [\"UserId\", \"Age\", \"Gender\", \"ProductId\", \"Score\"]\n",
    "weblogsWithUsers_hf = h2o_context.as_h2o_frame(result_final.select(final_columns), \"weblogsWithUsers\")\n",
    "weblogsWithUsers_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View H2OFrame in H2O Flow UI\n",
    "Once the create of the H2OFrame above is complete, open the H2O Flow UI page (`https://<ClusterName>-h2o.apps.azurehdinsight.net:443`).\n",
    "![H2O Flow Dashboard](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/h2o-flow-dashboard.png)\n",
    "\n",
    "From the H2O Flow dashboard, select `getFrames` to view the frames you loaded above. \n",
    "![H2O Flow getFrames](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/h2o-flow-get-frames.png)\n",
    "\n",
    "You should see a frame named `weblogsWithUsers` listed.\n",
    "![H2O Flow Frames](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/h2o-flow-frames.png)\n",
    "\n",
    "Select the `weblogsWithUsers` frame, and take a closer look at the data and options available in H2O Flow.\n",
    "![H2O Flow Frame Summary](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/h2o-flow-frame-summary.png)\n",
    "\n",
    "H2O Flow UI provides the capability to build models and predictions directly from the web UI. However, for this lab we are going to return to our Jupyter notebook, and create the model there, using Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training and test datasets\n",
    "Our next task is to create datasets to user for training and testing our model, as well as assign the categoricals needed for perform our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign factors\n",
    "Score, Age, and Gender columns need to be factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform select columns to categoricals\n",
    "weblogsWithUsers_hf[\"Score\"] = weblogsWithUsers_hf[\"Score\"].asfactor()\n",
    "weblogsWithUsers_hf[\"Age\"] = weblogsWithUsers_hf[\"Age\"].asfactor()\n",
    "weblogsWithUsers_hf[\"Gender\"] = weblogsWithUsers_hf[\"Gender\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the predictor names and response column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the predictor names and the response column name\n",
    "predictors = [\"Age\", \"Gender\"]\n",
    "response = \"Score\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation datasets for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split frame into two - we use one as the training frame and the second as the validation frame\n",
    "splits = weblogsWithUsers_hf.split_frame(ratios=[0.75], destination_frames=[\"train\", \"valid\"], seed=42)\n",
    "train_hf = splits[0]\n",
    "valid_hf = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a deep neural network to predict recommended products for a given user\n",
    "We are going to create an H2O deep learning model to perform our predictions with the following settings\n",
    "\n",
    "* model_id: \"ratingsModel\"\n",
    "* epochs: 0.25\n",
    "* activation: Tanh\n",
    "* hidden: [10, 10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "dl_model = H2ODeepLearningEstimator(model_id = \"ratingsModel\", activation = \"Tanh\", hidden = [10, 10, 10], epochs = 0.25)\n",
    "dl_model.train(x = predictors,\n",
    "               y = response,\n",
    "               training_frame = train_hf,\n",
    "               validation_frame = valid_hf)\n",
    "\n",
    "dl_model.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "Now, let's put our model to use, using our validation frame as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = dl_model.predict(valid_hf)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performance = dl_model.model_performance(valid_hf)\n",
    "performance.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Look at model and predictions in H2O Flow.\n",
    "We have completed creating a predictive model using H2O's Deep Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Solr\n",
    "![Apache Solr](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/apache-solr.png)\n",
    "\n",
    "Apache Solr is a third-party custom application, which has not been published to the Azure portal. Apache Solr is an enterprise search platform that enables powerful full-text search on data. While HDInsight enables storing and managing vast amounts of data, Apache Solr provides the capabilities to quickly retrieve the data.\n",
    "\n",
    "Solr will be installed using a Script Action on our HDInsight cluster.\n",
    "\n",
    "1. From your cluster blade in the Azure Portal, select **Script Actions**.\n",
    "    ![Configuration Script Action Link](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/config-script-action.png)\n",
    "2. Select **+ Submit New**.\n",
    "    ![Submit New Script Action](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/submit-new-script-action.png)\n",
    "3. From the Script type drop down, select **Install Solr**.\n",
    "    ![Submit New Script Action](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-install-script-action.png)\n",
    "4. Leave the default settings, and select **Create**.\n",
    "   \n",
    "After a few seconds, Solr will be installed. If you select Notifications in the Azure portal, you should a message like the following:\n",
    "![Submit New Script Action](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-install-success.png)\n",
    "\n",
    "That completes our installation of Solr! Now let's add some data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data to Solr\n",
    "Now that Solr is installed, let's add some sample data into it, and create an index for the data. To start this exercise, we will use sample data provided in the Solr installation, as Solr is configured by default with a specific schema that it expects data to conform to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To get at the sample data, we need to create an SSH tunnel to the head node of our cluster, where Solr is installed.\n",
    "\n",
    "1. Create SSH connection to the primary node of our cluster.\n",
    "    a. Use SSH to create an SSH tunnel to the cluster head node, using the following command: \n",
    "        > `ssh USERNAME@CLUSTERNAME-ssh.azurehdinsight.net`\n",
    "    b. Once connected, we are going to perform the following commands to copy the sample datainto Solr.\n",
    "        * `cd /usr/hdp/current/solr/example/exampledocs`\n",
    "        * java -jar post.jar solr.xml monitor.xml\n",
    "            ![Java jar output](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/java-jar-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Solr dashboard\n",
    "The Solr dashboard is a web UI that allows you to work with Solr through your web browser. The Solr dashboard is not exposed directly on the Internet, so you will need to use an SSH tunnel to access it.\n",
    "\n",
    "#### Determine the host name for the primary node.\n",
    "Using your SSH shell created above, type the following command to get the host name:\n",
    "```\n",
    "hostname -f\n",
    "```\n",
    "![hostname command](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/ssh-hostname-command.png)\n",
    "\n",
    "Copy the output from that command, as we will be using it below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure system to connect to Solr dashboard\n",
    "\n",
    "1. Open a new bash terminal window.\n",
    "2. Use SSH to create an SSH tunnel to the cluster head node, using the following command, replacing PORT with a random port number, and CLUSTERNAME with your cluster name.\n",
    "    ```\n",
    "    ssh -C2qTnNf -D PORT sshuser@CLUSTERNAME-ssh.azurehdinsight.net\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure proxy on local machine\n",
    "1. On your local machine, open your network settings, and create a SOCKS proxy for localhost and the PORT you specified in the connection above.\n",
    "    a. For Windows machines, go to Control Panel, Network and Internet, and then Internet Options.\n",
    "        * On the Internet Options dialog, select the Connections tab, then select LAN Settings.\n",
    "        * On the LAN Settings dialog, check Use a proxy server..., and select Advanced.\n",
    "        * On the Advanced dialog:\n",
    "            * Enter \"localhost\" in the Socks textbox, and your PORT value in the Port textbox.\n",
    "            * Select OK.\n",
    "        * Select Apply.\n",
    "    b. For Mac OS, open System Preferences, and select Network.\n",
    "        * Select Advanced on the Network dialog, then select the Proxies tab.\n",
    "        * Check SOCKS Proxy, enter localhost for the SOCKS Proxy Server.\n",
    "        * Enter your PORT in the port textbox, next to the SOCKS Proxy Server box.\n",
    "        * Select OK.\n",
    "        * Select Apply on the Network dialog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Solr dashboard\n",
    "In your browser, connect to `http://HOSTNAME:8983/solr/#/`, where HOSTNAME is the name you copied after executing the `hostname -f` command in a previous step.\n",
    "\n",
    "![Solr Dashboard](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View indexed data\n",
    "From the left menu, click the **Core Selector** drown-down.\n",
    "![Solr Core Selector](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-core-selector.png)\n",
    "\n",
    "Then, select collection 1.\n",
    "![Solr collection1](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-collection1.png)\n",
    "\n",
    "Under collection 1, select Query.\n",
    "![Solr query](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-query.png)\n",
    "\n",
    "Leave the default query settings, and select **Execute Query**\n",
    "![Solr Query Results](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-query-results.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents\n",
    "Now, let's add some of our own data. The below JSON is output from our products table, formatted in the shaped needed by Solr.\n",
    "\n",
    "```javascript\n",
    "[{\"id\":628,\"name\":\"High Heels\",\"price\":31.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":627,\"name\":\"Wedge Heel Shoes\",\"price\":27.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":626,\"name\":\"Ankle Boots\",\"price\":26.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":625,\"name\":\"Summer Sandal\",\"price\":25.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":624,\"name\":\"Canvas Boat Shoe\",\"price\":16.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":623,\"name\":\"Indoor Slipper\",\"price\":12.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":622,\"name\":\"High Heel Zip Snow Boots\",\"price\":35.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":621,\"name\":\"Knee High Boots\",\"price\":22.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":620,\"name\":\"Welly Boots\",\"price\":32.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":619,\"name\":\"Fully Fleece Lining Snow Boots\",\"price\":25.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":617,\"name\":\"Faux Fur Slipper\",\"price\":19.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"},\n",
    "{\"id\":616,\"name\":\"Faux Fur Snow Boot\",\"price\":25.0,\"cat\":\"31\",\"category\":\"Womens Casual Shoes\",\"sku\":\"Clothing\"}]\n",
    "```\n",
    "\n",
    "Select **Documents** in the collection 1 menu, and select Solr Command (raw XML or JSON) for the Document Type:\n",
    "![Solr Document Add](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-document-add.png)\n",
    "\n",
    "Then, paste the JSON text above into the Document(s) box, replacing any existing text.\n",
    "\n",
    "![Solr JSON Document Upload](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-json-document-insert.png)\n",
    "\n",
    "Click **Submit Document** to add the JSON documents to the search index.\n",
    "![Solr JSON Response](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab07/images/solr-json-insert-response.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the data\n",
    "Now, you can return to the query screen, and view the newly added documents in the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "### Proxy settings\n",
    "* Remove proxy settings for SSH tunnel set up to allow connection to Solr dashboard.\n",
    "\n",
    "### Stop Solr\n",
    "* Execute the following command to stop solr on the cluster\n",
    "    ```\n",
    "    sudo stop solr\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this lab you have learned how to extend an HDInsight cluster using third-party applications, adding valuable functionality to the AdventureWorks cluster. There are multiple applications available to extend HDInsight clusters in the Azure portal, and more can be developed by Microsoft, independent software vendors (ISV) or by yourself.\n",
    "\n",
    "During this lab you:\n",
    "* Installed a custom application, Apache Solr, using a Script Action, since it has not been published to the Azure portal.\n",
    "* Used Solr to index and search a small sample of product data.\n",
    "* Installed H2O Sparkling Water.\n",
    "* Created a deep learning model using Spark and H2O.\n",
    "* Used H2O Flow UI to view your model and predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}