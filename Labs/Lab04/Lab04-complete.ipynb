{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04 - Streaming Pattern - Processing events from Kafka using Spark and MLlib\n",
    "\n",
    "AdventureWorks has asked for the ability to extend their product recommendations feature, integrating the trained Alternating Least Squares (ALS) recommendation model to make predictions against streaming weblog data from Kafka.\n",
    "\n",
    "In this lab, you will upload and run a Java .jar application to add sample weblog data into a Kafka topic, and use the same application to view the data added. You will then create a simple Kafka producer using Spark to add a few more records to the topic. Next, you will use Spark Structured Streaming to query the data, and run the streamed data against the ALS recommendation model, getting product recommendations for a given user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* An Azure Virtual Network\n",
    "* A Spark 2.1 on HDInsight 3.6 cluster, inside the virtual network\n",
    "* A Kafka on HDInsight 3.6 cluster, inside the virtual network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "The first thing you need to do is prepare the environment for the tasks ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required packages\n",
    "To use Spark Structured Streaming with Kafka, you must load the appropriate packages. The version must match the version of both Kafka and Spark that you are using, so for our setup we need to load packages that works with Kafka on HDInsight 3.6, and Spark 2.1 on HDInsight 3.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0,org.apache.spark:spark-streaming_2.11:2.1.0,org.apache.spark:spark-streaming-kafka-0-8_2.10:2.1.0\",\n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install `jq` on the header node of the Spark cluster\n",
    "For several steps of this lab you will be working with JSON data from the command line. `jq` is a lightweight and flexible command-line JSON processor, which you will use to parse the JSON returned by the `curl` command below. To install `jq` do the following:\n",
    "\n",
    "1. Open an bash shell prompt, and connect to the head node of your Spark cluster using the following command, replacing SPARKCLUSTERNAME with the name of your Spark cluster. \n",
    "```bash\n",
    "ssh sshuser@SPARKCLUSTERNAME-ssh.azurehdinsight.net\n",
    "```\n",
    "2. You will be presented with a prompt that the authenticity of the host can't be established. Type `yes` to continue.\n",
    "3. Enter your admin password when prompted.\n",
    "4. Once your connection is established, enter the following at the ssh command prompt.\n",
    "```bash\n",
    "sudo apt -y install jq\n",
    "```\n",
    "\n",
    "Leave the bash shell open, as you will be using it again below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Kafka topic\n",
    "Now that the environment is ready, the next thing you need to do when working with Kafka is create a topic. A topic is a category or feed name to which records are published. This will be where your streaming data resides within the Kafka cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Zookeeper hosts\n",
    "Topics are registered in ZooKeeper, which means you must provide the **Zookeeper host** information for your Kafka cluster. To find the Zookeeper host information for your Kafka HDInsight cluster, you can use the Ambari REST API. The following cell retrieves this information using the the `curl` and `jq` utilities using a `%%bash` shell magic command.\n",
    "\n",
    "> While there may be more than two Zookeeper hosts for your cluster, you do not need to provide a full list of all hosts to clients. One or two is enough. In this case, we return two.\n",
    "\n",
    "The following cell generates a comma-delimited list containing two hosts, similar to the following example:\n",
    "```\n",
    "zk0-kafka.rwlvi5egublulm0bp55vont2af.xx.internal.cloudapp.net:2181,zk1-kafka.rwlvi5egublulm0bp55vont2af.xx.internal.cloudapp.net:2181\n",
    "```\n",
    "\n",
    "Before running the cell below:\n",
    "1. Replace the value of `KAFKACLUSTERNAME` with the name of your Kafka cluster.\n",
    "2. Replace the value of `PASSWORD` with the admin password of your Kafka cluster (default value assigned was \"Abc!1234567890\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "KAFKACLUSTERNAME=\"<repace with your kafka cluster name>\"\n",
    "PASSWORD=\"Abc!1234567890\"\n",
    "curl -su admin:$PASSWORD -G \"https://$KAFKACLUSTERNAME.azurehdinsight.net/api/v1/clusters/$KAFKACLUSTERNAME/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the topic\n",
    "Now that we have our Zookeeper host list, you can use a bash shell to create a topic on Kafka called \"weblogs.\"\n",
    "\n",
    "Copy the output from the previous step, and use it to replace the value of the `ZOOKEEPER_HOSTS` variable in the next cell.\n",
    "\n",
    "> For example, ZOOKEEPER_HOSTS=\"zk1-kafka.0qmwcwuospvenlxdqylbdkt1jc.xx.internal.cloudapp.net:2181,zk3-kafka.0qmwcwuospvenlxdqylbdkt1jc.xx.internal.cloudapp.net:2181\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TOPIC=\"weblogs\"\n",
    "ZOOKEEPER_HOSTS=\"<replace with output from previous command>\"\n",
    "/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic $TOPIC --zookeeper $ZOOKEEPER_HOSTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload and run a Kafka producer/consumer application\n",
    "The next step is to upload an application that can be used to write data to your Kafka topic. For this, you will use a Java application which produces sample weblog data, and writes it to the Kafka cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the `jar` file to the Spark cluster head node\n",
    "As part of the package you downloaded for this lab, you will find a file named kafka-producer-consumer.jar. This file needs to be uploaded to the head node of your Spark cluster. \n",
    "\n",
    "From a **new** bash shell prompt, you will upload the compiled `jar` file to the local storage of your Spark HDInsight cluster head node using an `scp` command. As done earlier, replace SPARKCLUSTERNAME with the name you provided earlier for your Spark cluster. When prompted, enter the password for the SSH user. Replace the \"/path/to/Kafka-Producer-Consumer/kafka-producer-consumer.jar\" with the path to this file in the Lab04 folder.\n",
    "\n",
    "```bash\n",
    "scp ./kafka-producer-consumer.jar sshuser@SPARKCLUSTERNAME-ssh.azurehdinsight.net:kafka-producer-consumer.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return to the shell you opened previously, which has the SSH connection to your Spark cluster head node.\n",
    "Back in the bash shell where you performed the `jq` installation, verify the `kafka-producer-consumer.jar` file was uploaded by listing the files.\n",
    "```bash\n",
    "ll\n",
    "```\n",
    "You should see output similar to the following image:\n",
    "![Head Node Files](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab04/images/cluster-file-list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get you Kafka brokers\n",
    "Before attempting to run the Kafka producer, you need to retrieve your **Kafka brokers**. These brokers provide the connection information needed for the kafka-producer-consumer command-line app to write and read records to and from your Kafka cluster. To find the Kafka broker information for your Kafka HDInsight cluster, you can use the Ambari REST API. The following cell retrieve this information using the the `curl` and `jq` utilities using a `%%bash` shell magic command.\n",
    "\n",
    "> While there may be more than two broker hosts for your cluster, you do not need to provide a full list of all hosts to clients. One or two is enough. In this case, we return two.\n",
    "\n",
    "The following cell generates a comma-delimited list containing two hosts, similar to the following example:\n",
    "```\n",
    "wn0-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn1-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092\n",
    "```\n",
    "\n",
    "Before running the cell below:\n",
    "1. Replace the value of `KAFKACLUSTERNAME` with the name of your Kafka cluster.\n",
    "2. Replace the value of `PASSWORD` with the admin password of your Kafka cluster (default value assigned was \"Abc!1234567890\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "KAFKACLUSTERNAME=\"<repace with your kafka cluster name>\"\n",
    "PASSWORD=\"Abc!1234567890\"\n",
    "curl -su admin:$PASSWORD -G \"https://$KAFKACLUSTERNAME.azurehdinsight.net/api/v1/clusters/$KAFKACLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Kafka Producer\n",
    "Return to your bash shell with the SSH connection to your Spark cluster head node.\n",
    "\n",
    "You will now execute a producer command in the application to write records to Kafka. The following command will write 100,000 abbreviated weblog records to your topic in Kafka. The records are in JSON format, and look like the following:\n",
    "\n",
    "```json\n",
    "{\"ProductId\" : 33, \"UserId\" : 37}\n",
    "{\"ProductId\" : 95, \"UserId\" : 2208}\n",
    "{\"ProductId\" : 83, \"UserId\" : 9316}\n",
    "{\"ProductId\" : 1, \"UserId\" : 7418}\n",
    "{\"ProductId\" : 92, \"UserId\" : 10569}\n",
    "```\n",
    "\n",
    "Replace the value of `KAFKABROKERS` with the output of the previous cell, then copy and paste the code into your bash shell.\n",
    "\n",
    "```bash\n",
    "TOPIC=weblogs\n",
    "KAFKABROKERS=\"<replace with your kafka brokers list>\"\n",
    "java -jar kafka-producer-consumer.jar producer $KAFKABROKERS $TOPIC\n",
    "```\n",
    "\n",
    "You should see the number of records written count up to 100,000 in the dialog.\n",
    "\n",
    "![Producer output](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab04/images/kafka-producer-app-output.png)\n",
    "\n",
    "> You may see a failure message, as is highlighted in the image above. This can be safely ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Kafka Consumer\n",
    "Now that you have successfully used the Kafka producer to write data to your topic, let's use the consumer component of the `jar` application to look at the data that you uploaded.\n",
    "\n",
    "Copy and paste the following code into your bash shell connected via SSH to your Spark cluster head node. Execution of this command will output the data written by the previous command.\n",
    "\n",
    "> Note the $KAFKABROKERS and $TOPIC variables don't need to be assigned, as they were added with the previous producer commands.\n",
    "\n",
    "```bash\n",
    "java -jar kafka-producer-consumer.jar consumer $KAFKABROKERS $TOPIC\n",
    "```\n",
    "\n",
    "You will notice that the command does not return to a command prompt. You can press `CTRL+C` to get the prompt back, but for now leave it as is.\n",
    "\n",
    "![Consumer output](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab04/images/kafka-consumer-app-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple Spark producer\n",
    "In this next section, you will use Spark as a Kafka Producer, and add a few more records to the topic. With the open Consumer prompt, you will be able to observe the new records being added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Weblogs data\n",
    "First, retrieve your weblogs data from the `weblogs` Hive table, and store it in a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use Spark SQL to retrieve all the records from the \"weblogs\" Hive table.\n",
    "val weblogs = spark.sql(\"SELECT * FROM weblogs\")\n",
    "weblogs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample dataset\n",
    "From the weblogs data, create a small sample of data you can send to Kafka.\n",
    "\n",
    "> The weblogs data contains approximately 90 million records, as well as mulitple fields that are not need for this exercise, so the sampel dataset will make things more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use the select and limit methods of a Spark Dataframe to create a new Dataframe\n",
    "// containing only 100 records, and only those fields we want to write to Kafka.\n",
    "val sample = weblogs.selectExpr(\"ProductId\", \"UserId\").limit(100)\n",
    "sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new Spark Kafka Producer\n",
    "To write records to Kafka from your Spark Dataframe, you first need to create a Kafka Producer. This will be very similar to the code found inside the Java kafka-producer-consumer.jar application executed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the properties for our Kafka Producer\n",
    "First, set the properties of the Kafka producer.\n",
    "\n",
    "Before executing the cell below:\n",
    "1. Replace the value of `kafkaBrokers` with your comma-delimited list of Kafka brokers from above.\n",
    "2. Replace the value of `kafkaTopic` with the name you provided in for your Kafka topic above. If you left the default value of `weblogs` you don't need to change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import required libraries\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import java.util.Properties;\n",
    "import scala.util.parsing.json.JSONObject\n",
    "\n",
    "val kafkaBrokers = \"<replace with your kafka broker list>\"\n",
    "val kafkaTopic = \"weblogs\"\n",
    "\n",
    "val props = new Properties()\n",
    "props.put(\"bootstrap.servers\", kafkaBrokers)\n",
    "props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "println(\"Finished configuring Kafka Producer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the producer stream\n",
    "In the next cell, you will start streaming weblog data into Kafka. For accommodate this, you will generate a KafkaProducer, and use its send method to pass each row of weblog data into Kafka.\n",
    "\n",
    "Once you start running the cell, return to your bash shell with the open consumer session, and you should see the new records streaming into the topic in Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create an accumulator to track the number of weblog entries emitted to Kafka\n",
    "val numRecords = sc.accumulator(0L,\"Weblog entries sent to Kafka\")\n",
    "\n",
    "def convertToJson(row: Row): String = {\n",
    "    val m = row.getValuesMap(row.schema.fieldNames)\n",
    "    JSONObject(m).toString()\n",
    "}\n",
    "\n",
    "// Loop through the records in the sample DataFrame, converting each row to JSON and passing it to a Kafka producer.\n",
    "for (rec <- sample.collect()) {\n",
    "    val producer = new KafkaProducer[String,String](props)\n",
    "    val jsonData = convertToJson(rec)\n",
    "    val message = new ProducerRecord[String,String](kafkaTopic, \"spark_demo\", jsonData)\n",
    "    producer.send(message)\n",
    "    producer.close()\n",
    "    numRecords +=1\n",
    "}\n",
    "\n",
    "println(\"Finished writting \" + numRecords + \" records to Kafka topic '\" + kafkaTopic + \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your bash shell, you should have seen the 100 new records streaming into the topic via the consumer application. The total records in the topic should now read 100100.\n",
    "\n",
    "![Spark Producer output](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab04/images/spark-producer-output.png)\n",
    "\n",
    "You can now press `CTRL+C` in the bash shell to exit the consumer app, and return to the command prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create query for the Kafka stream\n",
    "It is now time to look at reading the data from Kafka into a Spark Structured Streaming Dataframe.\n",
    "\n",
    "You can access the data stored in the Kafka topic, and read it into a Spark streaming DataFrame, by subscribing to your `weblogs` topic. The `kafka` DataFrame, below, represents an unbounded table containing the streaming data, and is your query for the streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Construct a streaming DataFrame that reads from weblogs\n",
    "val kafka = { spark.readStream.format(\"kafka\")\n",
    "             .option(\"kafka.bootstrap.servers\", kafkaBrokers)\n",
    "             .option(\"subscribe\", kafkaTopic)\n",
    "             .option(\"startingOffsets\", \"earliest\")\n",
    "             .load() }\n",
    "\n",
    "kafka.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the schema outuput for the `kakfa` streaming DataFrame, you can see it includes the fields `key`, `value`, `topic`, `partition`, `offset`, `timestamp` and `timestampType` fields. You can pick and choose the columns needed for processing. The `value` field contains the actual data, and `timestamp` is message arrival timestamp.\n",
    "\n",
    "For this lab, we are only interested in the `value` field. Notice how it is currently displayed as binary data. To make use of this field, you need to convert it to a string.\n",
    "\n",
    "The final step is to actually start receiving the data. To do this you can set it up to print the `value` field to the console, and then start the stream using `start()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert the value column to a string\n",
    "val kafka_value = kafka.select(col(\"value\").cast(\"string\"))\n",
    "\n",
    "kafka_value.selectExpr(\"value\").writeStream.format(\"console\").start.awaitTermination(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this code is executed, the streaming computation will have started in the background. The query object is a handle to that active streaming query, and we have decided to run the query for 10 seconds before termination, by using `awaitTermination(10000)`.\n",
    "\n",
    "As you can see, all of the data in the value column is in a JSON format. You will need to convert it to the appropriate data types, and create columns in a DataFrame for the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a schema for reading the JSON data.\n",
    "To accomplish this, you will first need to create a schema that can be applied to the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import libraries used for declaring schemas and working with JSON data\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Define the structure of our weblogs JSON document that is read from Kafka.\n",
    "val schema = { (new StructType)\n",
    "                  .add(\"ProductId\", IntegerType)\n",
    "                  .add(\"UserId\", LongType) }\n",
    "             \n",
    "schema.printTreeString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an appropriate schema, you can create a new streaming DataFrame which contains your extracted JSON data by using the Spark SQL `from_json` method. This will create an aliased column named \"weblog\" to store the object. Then, you can select the data into a console stream for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val weblog_data = kafka_value.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"weblog\"))\n",
    "val weblogs_stream = weblog_data.select(\"weblog.ProductId\", \"weblog.UserId\")\n",
    "\n",
    "// Output to the console, so you can view the data.\n",
    "weblogs_stream.writeStream.format(\"console\").start.awaitTermination(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a quick look at the isStreaming method. This is a way to verify whether or not a DataFrame is streaming. For our weblog_stream DataFrame, you should see true returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblogs_stream.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparision, run the same command against the weblogs DataFrame you created above to load the weblogs data from CSV files in Azure storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblogs.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an in-memory query for accessing the streaming data\n",
    "In this scenario, you will store the input data as an in-memory table. From here, you can query the dataset using SQL. The name of table is specified from the queryName option.\n",
    "\n",
    "> NOTE: In-memory output should only be used on small datasets, as the entire output is collected and stored in memory. In a production environment, you would want to write the query to a file or other sink using code like the following:\n",
    "```scala\n",
    "writeStream\n",
    "    .format(\"parquet\")        // can be \"orc\", \"json\", \"csv\", etc.\n",
    "    .option(\"path\", \"path/to/destination/dir\")\n",
    "    .start()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = { weblogs_stream\n",
    "        .writeStream\n",
    "        .format(\"memory\")\n",
    "        .queryName(\"streamingLogs\")\n",
    "        .start() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the in-memory query, you can now access the data via Spark SQL, using the queryName, `streamingLogs`, as the table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val user_product_mapping = spark.sql(\"select distinct ProductId, UserId from streamingLogs\")\n",
    "user_product_mapping.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalize the ML model\n",
    "Run the streaming DataFrames through the ALS model, so product recommendations based on the streaming datasets can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the model\n",
    "First, you need to ensure the model exists in the proper storage location. The output from the command below should resemble something like:\n",
    "```\n",
    "Found 3 items\n",
    "drwxr-xr-x   - livy supergroup          0 2017-10-23 18:16 /models/cfmodel/itemFactors\n",
    "drwxr-xr-x   - livy supergroup          0 2017-10-23 18:15 /models/cfmodel/metadata\n",
    "drwxr-xr-x   - livy supergroup          0 2017-10-23 18:16 /models/cfmodel/userFactors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /models/cfmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After verifying the model exists in storage, load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import mllib recommendation data types\n",
    "import org.apache.spark.ml.recommendation.ALSModel\n",
    "\n",
    "// Load the model using ASLModel's load method.\n",
    "val model = ALSModel.load(\"/models/cfmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to the streamed data from Kafka\n",
    "Now use the tranform method on the model to create a new DataFrame that includes all of the columns from our streamingLogs query, and adds a new prediciton column that indicates the \"confidence\" of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = model.transform(user_product_mapping)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prediction column, you may have NaN (not a number) values which simply mean no prediction. Let's clean up the prediction DataFrame by omitting rows with NaN values for the prediction, cache the results and take a peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val recommended_products = predictions.where(\"not isnan(prediction)\").orderBy(\"UserId\", \"prediction\")\n",
    "recommended_products.cache()\n",
    "recommended_products.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame for Products data\n",
    "Before running the model, let's load and parse the product data from Azure Storage, so it can be joined to the recommended_products data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val products_schema = { (new StructType)\n",
    "                       .add(\"ProductId\", IntegerType)\n",
    "                       .add(\"ProductName\", StringType)\n",
    "                       .add(\"Price\", FloatType)\n",
    "                       .add(\"CategoryId\", StringType)\n",
    "                       .add(\"Ignore1\", StringType)\n",
    "                       .add(\"Ignore2\", StringType)\n",
    "                       .add(\"Ignore3\", StringType)\n",
    "                       .add(\"Category\", StringType)\n",
    "                       .add(\"Department\", StringType) }\n",
    "\n",
    "val products_DF = { spark.read.format(\"com.databricks.spark.csv\")\n",
    "                   .option(\"header\", false)\n",
    "                   .schema(products_schema)\n",
    "                   .load(\"/retaildata/rawdata/ProductFile/part{*}\") }\n",
    "\n",
    "val products = products_DF.select(\"ProductId\", \"ProductName\", \"Price\", \"CategoryId\", \"Category\", \"Department\")\n",
    "products.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get product recommendations\n",
    "Execute the query below, joining the Products and Recommendation data. \n",
    "\n",
    "> If you don't get any results, try entering a different UserId in the where clause. You can select one from the results of the recommended_products.show() operation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_products.join(products, \"ProductId\").where(\"UserId = 807\").orderBy(col(\"prediction\").desc).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Stop the query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In the lab, you have learned how to use Spark Structured Streaming and Kafka to incorporate streaming data into a trained machine learning model.\n",
    "\n",
    "Specifically you:\n",
    "* Configured a Spark cluster to use Kafka\n",
    "* Created a Kafka topic\n",
    "* Used a Java application to add records to a Kafka topic, and to consume the records added\n",
    "* Created a simple Kafka Producer using Spark\n",
    "* Operationalized a trained ALS model to get product recommendations using streamed data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
