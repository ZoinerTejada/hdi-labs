{"nbformat_minor": 2, "cells": [{"source": "# Lab 04 - Streaming Pattern - Processing events from Kafka using Spark and MLlib\n\nAdventureWorks has asked for the ability to extend their product recommendations feature, integrating the trained Alternating Least Squares (ALS) recommendation model to make predictions against streaming weblog data from Kafka.\n\nIn this lab, you will upload and run a Java .jar application to add sample weblog data into a Kafka topic, and use the same application to view the data added. You will then create a simple Kafka producer using Spark to add a few more records to the topic. Next, you will use Spark Structured Streaming to query the data, and run the streamed data against the ALS recommendation model, getting product recommendations for a given user.", "cell_type": "markdown", "metadata": {}}, {"source": "## Requirements\n\n* An Azure Virtual Network\n* A Spark 2.1 on HDInsight 3.6 cluster, inside the virtual network\n* A Kafka on HDInsight 3.6 cluster, inside the virtual network", "cell_type": "markdown", "metadata": {}}, {"source": "## Environment setup\nThe first thing you need to do is prepare the environment for the tasks ahead.", "cell_type": "markdown", "metadata": {}}, {"source": "### Load required packages\nTo use Spark Structured Streaming with Kafka, you must load the appropriate packages. The version must match the version of both Kafka and Spark that you are using, so for our setup we need to load packages that works with Kafka on HDInsight 3.6, and Spark 2.1 on HDInsight 3.6.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0,org.apache.spark:spark-streaming_2.11:2.1.0,org.apache.spark:spark-streaming-kafka-0-8_2.10:2.1.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Install `jq` on the header node of the Spark cluster\nFor several steps of this lab you will be working with JSON data from the command line. `jq` is a lightweight and flexible command-line JSON processor, which you will use to parse the JSON returned by the `curl` command below. To install `jq` do the following:\n\n1. Open an bash shell prompt, and connect to the head node of your Spark cluster using the following command, replacing SPARKCLUSTERNAME with the name of your Spark cluster. \n```bash\nssh sshuser@SPARKCLUSTERNAME-ssh.azurehdinsight.net\n```\n2. You will be presented with a prompt that the authenticity of the host can't be established. Type `yes` to continue.\n3. Enter your admin password when prompted.\n4. Once your connection is established, enter the following at the ssh command prompt.\n```bash\nsudo apt -y install jq\n```\n\nLeave the bash shell open, as you will be using it again below.", "cell_type": "markdown", "metadata": {}}, {"source": "## Create a Kafka topic\nNow that the environment is ready, the next thing you need to do when working with Kafka is create a topic. A topic is a category or feed name to which records are published. This will be where your streaming data resides within the Kafka cluster.", "cell_type": "markdown", "metadata": {}}, {"source": "### Get Zookeeper hosts\nTopics are registered in ZooKeeper, which means you must provide the **Zookeeper host** information for your Kafka cluster. To find the Zookeeper host information for your Kafka HDInsight cluster, you can use the Ambari REST API. The following cell retrieves this information using the the `curl` and `jq` utilities using a `%%bash` shell magic command.\n\n> While there may be more than two Zookeeper hosts for your cluster, you do not need to provide a full list of all hosts to clients. One or two is enough. In this case, we return two.\n\nThe following cell generates a comma-delimited list containing two hosts, similar to the following example:\n```\nzk0-kafka.rwlvi5egublulm0bp55vont2af.xx.internal.cloudapp.net:2181,zk1-kafka.rwlvi5egublulm0bp55vont2af.xx.internal.cloudapp.net:2181\n```\n\nBefore running the cell below:\n1. Replace the value of `KAFKACLUSTERNAME` with the name of your Kafka cluster.\n2. Replace the value of `PASSWORD` with the admin password of your Kafka cluster (default value assigned was \"Abc!1234567890\").", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\nKAFKACLUSTERNAME=\"<repace with your kafka cluster name>\"\nPASSWORD=\"Abc!1234567890\"\ncurl -su admin:$PASSWORD -G \"https://$KAFKACLUSTERNAME.azurehdinsight.net/api/v1/clusters/$KAFKACLUSTERNAME/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Create the topic\nNow that we have our Zookeeper host list, you can use a bash shell to create a topic on Kafka called \"weblogs.\"\n\nCopy the output from the previous step, and use it to replace the value of the `ZOOKEEPER_HOSTS` variable in the next cell.\n\n> For example, ZOOKEEPER_HOSTS=\"zk1-kafka.0qmwcwuospvenlxdqylbdkt1jc.xx.internal.cloudapp.net:2181,zk3-kafka.0qmwcwuospvenlxdqylbdkt1jc.xx.internal.cloudapp.net:2181\"", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\nTOPIC=\"weblogs\"\nZOOKEEPER_HOSTS=\"<replace with output from previous command>\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic $TOPIC --zookeeper $ZOOKEEPER_HOSTS", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Upload and run a Kafka producer/consumer application\nThe next step is to upload an application that can be used to write data to your Kafka topic. For this, you will use a Java application which produces sample weblog data, and writes it to the Kafka cluster.", "cell_type": "markdown", "metadata": {}}, {"source": "### Copy the `jar` file to the Spark cluster head node\nAs part of the package you downloaded for this lab, you will find a file named kafka-producer-consumer.jar. This file needs to be uploaded to the head node of your Spark cluster. \n\nFrom a **new** bash shell prompt, you will upload the compiled `jar` file to the local storage of your Spark HDInsight cluster head node using an `scp` command. As done earlier, replace SPARKCLUSTERNAME with the name you provided earlier for your Spark cluster. When prompted, enter the password for the SSH user. Replace the \"/path/to/Kafka-Producer-Consumer/kafka-producer-consumer.jar\" with the path to this file in the Lab04 folder.\n\n```bash\nscp ./kafka-producer-consumer.jar sshuser@SPARKCLUSTERNAME-ssh.azurehdinsight.net:kafka-producer-consumer.jar\n```", "cell_type": "markdown", "metadata": {}}, {"source": "### Return to the shell you opened previously, which has the SSH connection to your Spark cluster head node.\nBack in the bash shell where you performed the `jq` installation, verify the `kafka-producer-consumer.jar` file was uploaded by listing the files.\n```bash\nll\n```\nYou should see output similar to the following image:\n![Head Node Files](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab04/images/cluster-file-list.png)", "cell_type": "markdown", "metadata": {}}, {"source": "### Get you Kafka brokers\nBefore attempting to run the Kafka producer, you need to retrieve your **Kafka brokers**. These brokers provide the connection information needed for the kafka-producer-consumer command-line app to write and read records to and from your Kafka cluster. To find the Kafka broker information for your Kafka HDInsight cluster, you can use the Ambari REST API. The following cell retrieve this information using the the `curl` and `jq` utilities using a `%%bash` shell magic command.\n\n> While there may be more than two broker hosts for your cluster, you do not need to provide a full list of all hosts to clients. One or two is enough. In this case, we return two.\n\nThe following cell generates a comma-delimited list containing two hosts, similar to the following example:\n```\nwn0-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092,wn1-kafka.liftazhqudlunpo4tkvapo234g.dx.internal.cloudapp.net:9092\n```\n\nBefore running the cell below:\n1. Replace the value of `KAFKACLUSTERNAME` with the name of your Kafka cluster.\n2. Replace the value of `PASSWORD` with the admin password of your Kafka cluster (default value assigned was \"Abc!1234567890\").", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\nKAFKACLUSTERNAME=\"<repace with your kafka cluster name>\"\nPASSWORD=\"Abc!1234567890\"\ncurl -su admin:$PASSWORD -G \"https://$KAFKACLUSTERNAME.azurehdinsight.net/api/v1/clusters/$KAFKACLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "### Execute the Kafka Producer\nReturn to your bash shell with the SSH connection to your Spark cluster head node.\n\nYou will now execute a producer command in the application to write records to Kafka. The following command will write 100,000 abbreviated weblog records to your topic in Kafka. The records are in JSON format, and look like the following:\n\n```json\n{\"ProductId\" : 33, \"UserId\" : 37}\n{\"ProductId\" : 95, \"UserId\" : 2208}\n{\"ProductId\" : 83, \"UserId\" : 9316}\n{\"ProductId\" : 1, \"UserId\" : 7418}\n{\"ProductId\" : 92, \"UserId\" : 10569}\n```\n\nReplace the value of `KAFKABROKERS` with the output of the previous cell, then copy and paste the code into your bash shell.\n\n```bash\nTOPIC=weblogs\nKAFKABROKERS=\"<replace with your kafka brokers list>\"\njava -jar kafka-producer-consumer.jar producer $KAFKABROKERS $TOPIC\n```\n\nYou should see the number of records written count up to 100,000 in the dialog.\n\n![Producer output](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab04/images/kafka-producer-app-output.png)\n\n> You may see a failure message, as is highlighted in the image above. This can be safely ignored.", "cell_type": "markdown", "metadata": {}}, {"source": "### Execute the Kafka Consumer\nNow that you have successfully used the Kafka producer to write data to your topic, let's use the consumer component of the `jar` application to look at the data that you uploaded.\n\nCopy and paste the following code into your bash shell connected via SSH to your Spark cluster head node. Execution of this command will output the data written by the previous command.\n\n> Note the $KAFKABROKERS and $TOPIC variables don't need to be assigned, as they were added with the previous producer commands.\n\n```bash\njava -jar kafka-producer-consumer.jar consumer $KAFKABROKERS $TOPIC\n```\n\nYou will notice that the command does not return to a command prompt. You can press `CTRL+C` to get the prompt back, but for now leave it as is.\n\n![Consumer output](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab04/images/kafka-consumer-app-output.png)", "cell_type": "markdown", "metadata": {}}, {"source": "## Create a simple Spark producer\nIn this next section, you will use Spark as a Kafka Producer, and add a few more records to the topic. With the open Consumer prompt, you will be able to observe the new records being added.", "cell_type": "markdown", "metadata": {}}, {"source": "### Get Weblogs data\nFirst, retrieve your weblogs data from the `weblogs` Hive table, and store it in a Spark DataFrame.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// TODO: Use Spark SQL to retrieve all the records from the \"weblogs\" Hive table.\nval weblogs = //TODO \nweblogs.printSchema()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Create sample dataset\nFrom the weblogs data, create a small sample of data you can send to Kafka.\n\n> The weblogs data contains approximately 90 million records, as well as mulitple fields that are not need for this exercise, so the sampel dataset will make things more manageable.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// TODO: Use the select and limit methods of a Spark Dataframe to create a new Dataframe\n//       containing only 100 records, and only those fields we want to write to Kafka.\n//       Hint: The fields needed can be seen in the output from the consume action we executed in the bash shell previously.\nval sample = weblogs.selectExpr //TODO ( )\nsample.show(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Create a new Spark Kafka Producer\nTo write records to Kafka from your Spark Dataframe, you first need to create a Kafka Producer. This will be very similar to the code found inside the Java kafka-producer-consumer.jar application executed above.", "cell_type": "markdown", "metadata": {}}, {"source": "#### Configure the properties for our Kafka Producer\nFirst, set the properties of the Kafka producer.\n\nBefore executing the cell below:\n1. Replace the value of `kafkaBrokers` with your comma-delimited list of Kafka brokers from above.\n2. Replace the value of `kafkaTopic` with the name you provided in for your Kafka topic above. If you left the default value of `weblogs` you don't need to change this.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Import required libraries\nimport org.apache.spark.sql._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\nimport java.util.Properties;\nimport scala.util.parsing.json.JSONObject\n\nval kafkaBrokers = \"<replace with your kafka broker list>\"\nval kafkaTopic = \"weblogs\"\n\nval props = new Properties()\nprops.put(\"bootstrap.servers\", kafkaBrokers)\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n\nprintln(\"Finished configuring Kafka Producer\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Start the producer stream\nIn the next cell, you will start streaming weblog data into Kafka. For accommodate this, you will generate a KafkaProducer, and use its send method to pass each row of weblog data into Kafka.\n\nOnce you start running the cell, return to your bash shell with the open consumer session, and you should see the new records streaming into the topic in Kafka.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Create an accumulator to track the number of weblog entries emitted to Kafka\nval numRecords = sc.accumulator(0L,\"Weblog entries sent to Kafka\")\n\ndef convertToJson(row: Row): String = {\n    val m = row.getValuesMap(row.schema.fieldNames)\n    JSONObject(m).toString()\n}\n\n// Loop through the records in the sample DataFrame, converting each row to JSON and passing it to a Kafka producer.\nfor (rec <- sample.collect()) {\n    val producer = new KafkaProducer[String,String](props)\n    val jsonData = convertToJson(rec)\n    val message = new ProducerRecord[String,String](kafkaTopic, \"spark_demo\", jsonData)\n    producer.send(message)\n    producer.close()\n    numRecords +=1\n}\n\nprintln(\"Finished writting \" + numRecords + \" records to Kafka topic '\" + kafkaTopic + \"'\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In your bash shell, you should have seen the 100 new records streaming into the topic via the consumer application. The total records in the topic should now read 100100.\n\n![Spark Producer output](https://raw.githubusercontent.com/ZoinerTejada/hdi-labs/master/Labs/Lab04/images/spark-producer-output.png)\n\nYou can now press `CTRL+C` in the bash shell to exit the consumer app, and return to the command prompt.", "cell_type": "markdown", "metadata": {}}, {"source": "## Read data from the Kafka stream...\nIt is now time to look at reading the data from Kafka into a Spark Structured Streaming Dataframe.\n\nYou can access the data stored in the Kafka topic, and read it into a Spark streaming DataFrame, by subscribing to your `weblogs` topic.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Construct a streaming DataFrame that reads from weblogs\nval kafka = { spark.readStream.format(\"kafka\")\n             .option(\"kafka.bootstrap.servers\", kafkaBrokers)\n             .option(\"subscribe\", kafkaTopic)\n             .option(\"startingOffsets\", \"earliest\")\n             .load() }\n\nkafka.printSchema()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Looking at the schema outuput for the `kakfa` streaming DataFrame, you can see it includes the fields `key`, `value`, `topic`, `partition`, `offset`, `timestamp` and `timestampType` fields. You can pick and choose the columns needed for processing. The `value` field contains the actual data, and `timestamp` is message arrival timestamp.\n\nFor this lab, we are only interested in the `value` field. Notice how it is currently displayed as binary data. To make use of this field, you need to convert it to a string.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// TODO: Convert the value column to a string\nval kafka_value = kafka.select(col(\"value\").//TODO)\n\nkafka_value.selectExpr(\"value\").writeStream.format(\"console\").start.awaitTermination(10000)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "As you can see, all of the data in the value column is in a JSON format. You will need to convert it to the appropriate data types, and create columns in a DataFrame for the values.", "cell_type": "markdown", "metadata": {}}, {"source": "### Create a schema for reading the JSON data.\nTo accomplish this, you will first need to create a schema that can be applied to the JSON data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Import libraries used for declaring schemas and working with JSON data\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\n// Define the structure of our weblogs JSON document that is read from Kafka.\nval schema = { (new StructType)\n                  .add(\"ProductId\", IntegerType)\n                  .add(\"UserId\", LongType) }\n             \nschema.printTreeString", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now that you have an appropriate schema, you can create a new streaming DataFrame which contains your extracted JSON data by using the Spark SQL `from_json` method. This will create an aliased column named \"weblog\" to store the object. Then, you can select the data into a console stream for viewing.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val weblog_data = kafka_value.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"weblog\"))\nval weblogs_stream = weblog_data.select(\"weblog.ProductId\", \"weblog.UserId\")\n\n// Output to the console, so you can view the data.\nweblogs_stream.writeStream.format(\"console\").start.awaitTermination(10000)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now, let's take a quick look at the isStreaming method. This is a way to verify whether or not a DataFrame is streaming. For our weblog_stream DataFrame, you should see true returned.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "weblogs_stream.isStreaming", "outputs": [], "metadata": {"collapsed": false}}, {"source": "For comparision, run the same command against the weblogs DataFrame you created above to load the weblogs data from CSV files in Azure storage.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "weblogs.isStreaming", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Create an in-memory query for accessing the streaming data\nIn this scenario, you will store the input data as an in-memory table. From here, you can query the dataset using SQL. The name of table is specified from the queryName option.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val query = { weblogs_stream\n        .writeStream\n        .format(\"memory\")\n        .queryName(\"streamingLogs\")\n        .start() }", "outputs": [], "metadata": {"collapsed": false}}, {"source": "With the in-memory query, you can now access the data via Spark SQL, using the queryName, `streamingLogs`, as the table name.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val user_product_mapping = spark.sql(\"select distinct ProductId, UserId from streamingLogs\")\nuser_product_mapping.show(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Operationalize the ML model\nRun the streaming DataFrames through the ALS model, so product recommendations based on the streaming datasets can be generated.", "cell_type": "markdown", "metadata": {}}, {"source": "### Retrieve the model\nFirst, you need to ensure the model exists in the proper storage location. The output from the command below should resemble something like:\n```\nFound 3 items\ndrwxr-xr-x   - livy supergroup          0 2017-10-23 18:16 /models/cfmodel/itemFactors\ndrwxr-xr-x   - livy supergroup          0 2017-10-23 18:15 /models/cfmodel/metadata\ndrwxr-xr-x   - livy supergroup          0 2017-10-23 18:16 /models/cfmodel/userFactors\n```", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%sh\nhdfs dfs -ls /models/cfmodel", "outputs": [], "metadata": {"collapsed": false}}, {"source": "After verifying the model exists in storage, load the model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Import mllib recommendation data types\nimport org.apache.spark.ml.recommendation.ALSModel\n\n// TODO: Load the model using ASLModel's `load` method.\nval model = ALSModel.//TODO ( )", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Apply the model to the streamed data from Kafka\nNow use the tranform method on the model to create a new DataFrame that includes all of the columns from our streamingLogs query, and adds a new prediciton column that indicates the \"confidence\" of the prediction.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val predictions = model.transform(user_product_mapping)\npredictions.show(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In the prediction column, you may have NaN (not a number) values which simply mean no prediction. Let's clean up the prediction DataFrame by omitting rows with NaN values for the prediction, cache the results and take a peek.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val recommended_products = predictions.where(\"not isnan(prediction)\").orderBy(\"UserId\", \"prediction\")\nrecommended_products.cache()\nrecommended_products.show(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Create a DataFrame for Products data\nBefore running the model, let's load and parse the product data from Azure Storage, so it can be joined to the recommended_products data. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val products_schema = { (new StructType)\n                       .add(\"ProductId\", IntegerType)\n                       .add(\"ProductName\", StringType)\n                       .add(\"Price\", FloatType)\n                       .add(\"CategoryId\", StringType)\n                       .add(\"Ignore1\", StringType)\n                       .add(\"Ignore2\", StringType)\n                       .add(\"Ignore3\", StringType)\n                       .add(\"Category\", StringType)\n                       .add(\"Department\", StringType) }\n\nval products_DF = { spark.read.format(\"com.databricks.spark.csv\")\n                   .option(\"header\", false)\n                   .schema(products_schema)\n                   .load(\"/retaildata/rawdata/ProductFile/part{*}\") }\n\nval products = products_DF.select(\"ProductId\", \"ProductName\", \"Price\", \"CategoryId\", \"Category\", \"Department\")\nproducts.show(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Get product recommendations\nExecute the query below, joining the Products and Recommendation data. \n\n> If you don't get any results, try entering a different UserId in the where clause. You can select one from the results of the recommended_products.show() operation above.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "recommended_products.join(products, \"ProductId\").where(\"UserId = 807\").orderBy(col(\"prediction\").desc).show(10)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Conclusion\nIn the lab, you have learned how to use Spark Structured Streaming and Kafka to incorporate streaming data into a trained machine learning model.\n\nSpecifically you:\n* Configured a Spark cluster to use Kafka\n* Created a Kafka topic\n* Used a Java application to add records to a Kafka topic, and to consume the records added\n* Created a simple Kafka Producer using Spark\n* Operationalized a trained ALS model to get product recommendations using streamed data", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}
