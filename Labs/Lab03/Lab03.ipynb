{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Data Science using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdventureWorks would like to add a snazzy product recommendations feature to their website and email marketing campaigns that, for every user in their system, can recommend the top 10 products they might be interested in purchasing.\n",
    "\n",
    "Adventureworks has provided you with the tables for users, products and weblogs that contains all of the data you need. You will train a recommendation model using Spark's built-in collaborative filtering alogrithm - [Alternating Least Squares (ALS)](http://spark.apache.org/docs/2.1.0/mllib-collaborative-filtering.html). Then you will use the model to pre-compute the user to product recommendation for every user and save this in a table. Then you will query from this table to quickly get the 10 product recommendations for a given user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the Weblogs Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the modules and functions we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import UserDefinedFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weblogs table we have tells us what actions a user took on any given product when using the website. A user can browse a product, add it to the cart or purchase it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT UserId,  ProductId, Action FROM weblogs limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training and test data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by selecting a significant subset of our data to use in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = spark.sql(\"select * from weblogs where cleanedtransactiondate between '2016-03-01' and '2016-05-31'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining how we want to weigh the implicit rating described by the action field in the weblogs table. An implicit rating occurs here because a user is not explictly providing a rating (e.g., they never say \"I rate this product 4 out of 5 stars\". Instead we will infer their rating by virtue of their action. \n",
    "\n",
    "A product that is browsed gets 30 points, a product that is added to the cart gets 70 points and a product that is purchased gets 100 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ActionPoints = {\"Browsed\":30, \"Add To Cart\":70, \"Purchased\":100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new DataFrame that contains the a tuple with only the data we are interested in plus the value of the action taken. So our ratings will include the UserId, the ProductId and the Points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapActionToPoints = UserDefinedFunction(lambda a: ActionPoints[a], IntegerType())\n",
    "ratings_df = train.select(train.UserId, train.ProductId, mapActionToPoints(train.Action).alias('ActionPoints'))  \n",
    "ratings_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a model using ALS, we should cache the DataFrame because algorithm will revist the dataset many times over during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a model using ALS there are some settings referred to as hyperparameters that we need to use guide how the model gets trained. Typically you find the best values by iterating through a range of possible values and evaluating the predictive result. For simplicity, we will begin with the settings provided.\n",
    "\n",
    "We will train our data using the training data set and our hyperparameters. **Note that this will take about 1-2 minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Invoke the fit function of the ALS object providing the training DataFrame\n",
    "als = ALS(maxIter=10, regParam=0.01, userCol=\"UserId\", itemCol=\"ProductId\", ratingCol=\"ActionPoints\")\n",
    "model = als.#TODO( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you noticed, training a model takes some time. Fortunately, we don't have pay the training cost everytime we want to use the model. To allow us to re-use a trained model, we can save it to disk (in Azure Storage).\n",
    "\n",
    "With the folder structure in place, we need to invoke the save method on the model and indicate the path to the folder we created for it. You an store the models anywhere you want, but we chose /models/cfmodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Invoke the save method on the model object, providing the path in which to serialize the model.\n",
    "model.write().overwrite().#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's confirm our model was saved succesfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /models/cfmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a temporary view for the products data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load and parse the product data from Azure Storage and present it using a temporary view called Products_View. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_schema = StructType([\n",
    "        StructField('ProductId',IntegerType(),False), \n",
    "        StructField('ProductName', StringType()), \n",
    "        StructField('Price', FloatType()), \n",
    "        StructField('CategoryId', StringType()), \n",
    "        StructField('Ignore1', StringType()), \n",
    "        StructField('Ignore2', StringType()), \n",
    "        StructField('Ignore3', StringType()), \n",
    "        StructField('Category', StringType()), \n",
    "        StructField('Department', StringType())\n",
    "    ])\n",
    "\n",
    "products_DF = spark.read.csv(\"/retaildata/rawdata/ProductFile/part{*}\", \n",
    "                    schema=products_schema,\n",
    "                    header=False)\n",
    "\n",
    "products_DF_with_price = products_DF.select(\"ProductId\", \"ProductName\", \"Price\", \"CategoryId\", \"Category\", \"Department\")\n",
    "products_DF_with_price.printSchema()\n",
    "products_DF_with_price.show(5)\n",
    "\n",
    "products_DF_with_price.createOrReplaceTempView(\"Products_View\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put our model to use. We'll begin by creating a DataFrame that contains the set of UserID and ProductID for which we want to predict a rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_users_df = spark.sql(\"select distinct w.UserId, p.ProductId from weblogs w cross join Products_View p where w.UserId > 3686 and w.UserId < 3706\")\n",
    "test_users_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the tranform method on the model to create a new DataFrame that includes all of the columns from our test_users_df and adds a new prediciton column that indicates the \"confidence\" of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Invok the transform method of the model on the test data for which we want to acquire predictions.\n",
    "predictions = model.#TODO\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prediction column, we may have Nan (not a number) values which simply mean no prediction. Let's clean up our prediction DataFrame by omitting rows with NaN values for the prediction, cache the results and take a peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: filter out prediction values of \"NaN\" with the isnan() SQL function\n",
    "user_product_ratings = predictions.select(\"UserId\", \"ProductId\", \"prediction\").where(\"not #TODO\").orderBy(\"UserId\", predictions.prediction.desc())\n",
    "user_product_ratings.cache()\n",
    "user_product_ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll register this DataFrame as a temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_product_ratings.createOrReplaceTempView(\"UserProductRatings_View\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the top 10 recommended products for given user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our product data available as a view, we now have all of the data sources we need to start making recommendations: Products_View, the UserProductRatings_View and the Users table. We want to define a function that will get us the top 10 recommended products for a given user (by user ID) by querying our data sources. We'll start by building the core lines of the function individually and then put them together into a function we can easily invoke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's query the UserProductRatings_View for a sample user and examine what products our model suggests we recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_product_mapping = spark.sql(\"SELECT * FROM UserProductRatings_View WHERE UserId =\" + str(3696))\n",
    "user_product_mapping.show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets enrich the recommendation by joining the output with the products_DF_with_price DataFrame so that we can see the human friendly product names, category names and department names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommended_products = user_product_mapping.join(\n",
    "        products_DF_with_price, user_product_mapping.ProductId == products_DF_with_price.ProductId\n",
    ").orderBy( user_product_mapping.prediction.desc() )\n",
    "\n",
    "recommended_products.where(\"UserId = 3696\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put those lines together into a simple function we can call with a User ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO 1: complete the missing line\n",
    "#TODO 2: complete the missing line\n",
    "def GetRecommendedProductsForUser(UserId):\n",
    "    user_product_mapping = #TODO 1\n",
    "    recommended_products = #TODO 2\n",
    "    print(\"Users Information:\")\n",
    "    users_data = spark.sql(\"SELECT FirstName, LastName, Gender, Age from users WHERE id =\" + str(UserId))\n",
    "    users_data.show(1)\n",
    "    print(\"Recommended Products:\")\n",
    "    recommended_products.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll invoke the function next for our example user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GetRecommendedProductsForUser(UserId = 3696)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret these results. If you look at the prediction column, the values for this user span a range, whereby the higher the rating, the more confident we are of the recommendation. \n",
    "\n",
    "So now we need to ask ourselves, do these recommendation make sense for our example customer? Let's look at how we might answer this question in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by getting a sense for what items the customer buys or strongly considers buying (by adding them to his or her cart). We can run the following query. When the output is displayed, select Pie to create a pie chart and set X to Department and Y to count(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select w.Action, p.Department, p.Category, count(*)\n",
    "from weblogs w \n",
    "join products_view p on w.ProductId = p.ProductId\n",
    "where UserId = 3696 and Action = \"Purchased\" or Action = \"Add To Cart\"\n",
    "group by w.Action, p.Department, p.Category\n",
    "order by w.Action desc, count(*) desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the PieChart that we can readily see that Houseware, Clothing, Appliance, Toy, Outdoor & Garden and Auto make up this customers top 6 favorite departments of products. As a first pass to evalauting the model, this lends support to the notion that our recommender is choosing products from the right departments for our customer when it suggests products from Clothing, Auto and Appliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you learned how to perform collaborative filtering on a fairly large dataset and in the process, helped AdventureWorks recommend products to its users based on their activity on the website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
